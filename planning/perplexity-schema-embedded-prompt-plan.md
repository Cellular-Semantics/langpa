# Perplexity Schema-Embedded Prompt Implementation Plan

## Problem Statement

Perplexity is proving extremely finicky about what prompts can induce it to produce JSON output. The current langpa implementation embeds the JSON schema in the system prompt, but testing in the deep-research-client repo has revealed that embedding the full schema in the **user prompt** (not system prompt) produces more reliable JSON responses.

## Working Example Analysis

The working prompt pattern is found in:
- `deep-research-client/tmp/prompt_0_Gliosis.md` (and other prompt_*.md files)
- Generated by: `deep-research-client/tmp/generate_table_s10_prompts.py`

### Key Differences from Current langpa Approach

| Aspect | Working Pattern (deep-research-client) | Current langpa Approach |
|--------|---------------------------------------|------------------------|
| **Schema Location** | Full JSON schema embedded in user prompt (lines 23-235) | Summarized schema in system_prompt |
| **Gene List Format** | JSON array: `["SERPINE1", "EMP1", ...]` | Comma-separated string: `SERPINE1, EMP1, ...` |
| **Output Instruction** | Explicit: "Respond with JSON conforming to the provided schema - no prose, no markdown. If unable, use tables/fragments..." | Generic: "Respond ONLY with valid JSON..." |
| **System Prompt** | Not shown in examples (likely minimal or absent) | Contains full schema + validation instructions |

### Critical Success Factors

1. **Schema in User Message**: The complete JSON schema must appear in the user prompt, not system prompt
2. **Explicit Fallback Instructions**: Clear guidance on what to do if pure JSON fails
3. **JSON Array Gene Format**: Genes formatted as proper JSON array, not comma-separated string
4. **Minimal System Prompt**: System prompt should be minimal when schema is in user message

## Implementation Plan

### Phase 1: Add New Prompt Template

**File**: `langpa/src/langpa/services/deepsearch_prompts.py`

**Action**: Add new template to `PROMPT_TEMPLATES` dictionary:

```python
"gene_analysis_schema_embedded": {
    "template": """Perform comprehensive literature analysis for the following gene list in the specified biological context.

**Gene List**: {genes_json_array}

**Biological Context**: {context}

**Analysis Strategy**:
1. Search current scientific literature for functional roles of each gene in the input list
2. Identify clusters of genes that act together in pathways, processes, or cellular states
3. Treat each cluster as a potential gene program within the list
4. Interpret findings in light of both normal physiological roles and disease-specific alterations
5. Prioritize well-established functions with strong literature support, but highlight emerging evidence if contextually relevant.

**Guidelines**:
* Anchor all predictions in either the normal physiology and development of the cell type and tissue specified in the context OR the alterations and dysregulations characteristic of the specified disease
* Connect gene-level roles to program-level implications
* Consider gene interactions, regulatory networks, and pathway dynamics
* Highlight cases where multiple genes collectively strengthen evidence
* Ensure all claims are backed by experimental evidence with proper attribution

**Output**: Respond with JSON conforming to the provided schema - no prose, no markdown.  If you are unable to respond with JSON alone, make sure the text includes tables or fragments of schema compliant that unambiguously capture the elements and their associations that would be captured if returning JSON schema.

```json
{schema}
```
""",
    "supports_json_schema": True,
    "requires_full_schema": True,  # NEW FLAG indicating schema needs to be in user prompt
    "optimized_for": ["perplexity"],
    "description": "Schema-embedded prompt with full JSON schema in user message (proven to work reliably with Perplexity)"
}
```

### Phase 2: Update Template Formatting Function

**File**: `langpa/src/langpa/services/deepsearch_prompts.py`

**Action**: Modify `format_prompt_template()` to handle schema parameter:

```python
def format_prompt_template(
    template_name: str,
    genes: list[str],
    context: str,
    schema: dict | None = None  # NEW PARAMETER
) -> str:
    """Format a prompt template with genes, context, and optional schema.

    Args:
        template_name: Name of the template to use
        genes: List of gene symbols to analyze
        context: Biological context for the analysis
        schema: Optional JSON schema dict (required for schema-embedded templates)

    Returns:
        Formatted prompt string ready for API call

    Raises:
        ValueError: If template requires schema but none provided
    """
    import json

    template_config = get_prompt_template(template_name)
    template_text = template_config["template"]

    # Format genes as comma-separated string (default/legacy)
    genes_str = ", ".join(genes)

    # Format genes as JSON array (for schema-embedded templates)
    genes_json_array = json.dumps(genes)

    # Prepare substitution dict
    format_dict = {
        "genes": genes_str,
        "genes_json_array": genes_json_array,
        "context": context
    }

    # Add schema if template requires it
    if template_config.get("requires_full_schema", False):
        if schema is None:
            raise ValueError(
                f"Template '{template_name}' requires schema parameter. "
                "Schema must be provided when using schema-embedded templates."
            )
        format_dict["schema"] = json.dumps(schema, indent=2)

    # Substitute placeholders
    formatted_prompt = template_text.format(**format_dict)

    return formatted_prompt
```

**Also add** helper function to export template metadata:

```python
def get_template_metadata(template_name: str) -> dict[str, Any]:
    """Get metadata about a specific template.

    Args:
        template_name: Name of the template

    Returns:
        Dictionary with template metadata

    Raises:
        ValueError: If template_name is not found
    """
    template_config = get_prompt_template(template_name)

    return {
        "supports_json_schema": template_config.get("supports_json_schema", False),
        "requires_full_schema": template_config.get("requires_full_schema", False),  # NEW
        "optimized_for": template_config.get("optimized_for", []),
        "description": template_config.get("description", ""),
    }
```

### Phase 3: Update DeepSearchService

**File**: `langpa/src/langpa/services/deepsearch_service.py`

**Action 1**: Import the new metadata function at top:

```python
from langpa.services.deepsearch_prompts import (
    format_prompt_template,
    list_available_templates,
    get_template_metadata,  # ADD THIS IMPORT
)
```

**Action 2**: Modify `_construct_prompt()` method (around line 119):

```python
def _construct_prompt(
    self, genes: list[str], context: str, template_override: str | None = None
) -> str:
    """Construct the research prompt for gene list analysis.

    Uses configurable prompt templates for different analysis approaches.

    Args:
        genes: List of gene symbols to analyze
        context: Biological context for the analysis
        template_override: Optional template name to override config default

    Returns:
        Formatted prompt for DeepSearch API
    """
    # Determine which template to use
    template_name = template_override or self.config.prompt_template

    # Check if template requires schema embedding in user prompt
    template_metadata = get_template_metadata(template_name)
    schema = None
    if template_metadata.get("requires_full_schema", False):
        # Load schema for embedding in user prompt
        schema = load_schema("deepsearch_results_schema.json")

    # Format the prompt using the template system
    return format_prompt_template(template_name, genes, context, schema=schema)
```

**Action 3**: Modify `research_gene_list()` method to handle dual system prompt approaches (around line 194):

```python
# Starting at line 187 in research_gene_list()
try:
    # Load schema for response_format
    schema = load_schema("deepsearch_results_schema.json")

    # Prepare provider params from config
    provider_params = dict(self.config.provider_params)

    # Determine system prompt strategy based on template
    template_name = prompt_template or self.config.prompt_template
    template_metadata = get_template_metadata(template_name)

    if template_metadata.get("requires_full_schema", False):
        # Schema-embedded approach: minimal system prompt
        # Schema is in user prompt, so just remind model to return JSON only
        provider_params["system_prompt"] = """You are an expert biologist. Respond ONLY with valid JSON matching the schema provided in the user prompt. Do not include any prose, markdown, explanatory text, or <think> tags. Only output the JSON structure."""
    else:
        # Legacy approach: schema in system prompt
        provider_params["system_prompt"] = f"""You are an expert biologist. Analyze the provided genes in the given biological context.

CRITICAL: Respond ONLY with valid JSON that exactly follows this schema structure:
{json.dumps(schema, indent=2)}

Ensure every citation object includes "source_id" matching DeepSearch/Perplexity numbering.

Do not include any prose, markdown, explanatory text, or <think> tags. Only the JSON structure."""

    # Use configuration-driven research call
    result = self.client.research(
        query=prompt,
        provider=research_provider,
        model=self.config.model,
        provider_params=provider_params,
    )
    return result
```

### Phase 4: Add New Configuration Preset

**File**: `langpa/src/langpa/services/deepsearch_configs.py`

**Action**: Add new preset to `PRESET_CONFIGS` dictionary:

```python
"perplexity-sonar-schema-embedded": DeepSearchConfig(
    provider="perplexity",
    model="sonar-reasoning-pro",
    provider_params={
        "return_citations": True,
        "search_domain_filter": [
            "pubmed.ncbi.nlm.nih.gov",
            "ncbi.nlm.nih.gov/pmc/",
            "www.ncbi.nlm.nih.gov",
            "europepmc.org",
            "biorxiv.org",
            "nature.com",
            "cell.com",
            "science.org",
        ],
        "reasoning_effort": "high",
        "search_recency_filter": "month",
        "system_prompt": None,  # Will be set dynamically to minimal prompt
    },
    timeout=180,
    prompt_template="gene_analysis_schema_embedded",  # Use new template by default
    description="Schema-embedded approach with full schema in user prompt - proven to work reliably with Perplexity for JSON output"
),
```

### Phase 5: Testing & Validation

**Test 1: Basic JSON Output**
```bash
cd langpa
python scripts/run_deepsearch.py \
  --genes SERPINE1,EMP1,SPOCD1,ARHGAP29,IL1R1 \
  --context "malignant glioblastoma cells" \
  --preset perplexity-sonar-schema-embedded \
  --project perplexity_test \
  --query test_schema_embedded
```

**Test 2: Compare with Legacy Approach**
```bash
# Run with old approach
python scripts/run_deepsearch.py \
  --genes SERPINE1,EMP1,SPOCD1,ARHGAP29,IL1R1 \
  --context "malignant glioblastoma cells" \
  --preset perplexity-sonar-pro \
  --project perplexity_test \
  --query test_legacy

# Compare outputs
# Check which approach produces valid JSON more reliably
```

**Test 3: Batch Processing**
```bash
# Use one of the existing batch inputs to test at scale
python scripts/run_deepsearch.py \
  --batch-dir projects/glioblastoma/inputs \
  --preset perplexity-sonar-schema-embedded \
  --replicates 3
```

**Validation Checklist**:
- [ ] JSON is produced without markdown code blocks
- [ ] JSON validates against deepsearch_results_schema.json
- [ ] Gene list is correctly extracted into `input_genes` array
- [ ] Context is correctly parsed into `context.cell_type`, `context.disease`, `context.tissue`
- [ ] Programs array contains valid entries with all required fields
- [ ] Citations use proper `source_id` format matching Perplexity numbering

## Usage Examples

### Example 1: Using New Preset (Recommended)
```bash
python scripts/run_deepsearch.py \
  --genes TP53,BRCA1,PTEN,AKT1 \
  --context "breast cancer tumor cells" \
  --preset perplexity-sonar-schema-embedded
```

### Example 2: Using Template with Existing Preset
```bash
python scripts/run_deepsearch.py \
  --genes TP53,BRCA1,PTEN,AKT1 \
  --context "breast cancer tumor cells" \
  --preset perplexity-sonar-pro \
  --template gene_analysis_schema_embedded
```

### Example 3: Dry Run to See Generated Prompt
```bash
python scripts/run_deepsearch.py \
  --genes TP53,BRCA1 \
  --context "cancer cells" \
  --preset perplexity-sonar-schema-embedded \
  --dry-run
```

### Example 4: List Available Options
```bash
# See all presets including new one
python scripts/run_deepsearch.py --list-presets

# See all templates including new one
python scripts/run_deepsearch.py --list-templates

# Show details of new preset
python scripts/run_deepsearch.py --show-preset perplexity-sonar-schema-embedded

# Show details of new template
python scripts/run_deepsearch.py --show-template gene_analysis_schema_embedded
```

## Backward Compatibility

All changes maintain backward compatibility:

1. **Existing templates continue to work**: Templates without `requires_full_schema` flag use the legacy system prompt approach
2. **Existing presets unchanged**: `perplexity-sonar-pro` and other presets continue to use `gene_analysis_academic` template by default
3. **Optional schema parameter**: `format_prompt_template()` has schema as optional parameter, only required when template sets `requires_full_schema: True`
4. **Dual system prompt logic**: Code checks template metadata and applies appropriate system prompt strategy

## Rollout Strategy

### Phase 1: Implementation (Week 1)
- [ ] Implement code changes in development branch
- [ ] Add unit tests for new template formatting
- [ ] Add integration test with mock Perplexity response

### Phase 2: Testing (Week 1-2)
- [ ] Run side-by-side comparison tests (new vs legacy)
- [ ] Test with various gene list sizes (5, 20, 50, 100 genes)
- [ ] Test with different biological contexts
- [ ] Measure JSON success rate

### Phase 3: Documentation (Week 2)
- [ ] Update README with new preset information
- [ ] Add troubleshooting guide for JSON output issues
- [ ] Document when to use schema-embedded vs legacy approach

### Phase 4: Gradual Adoption (Week 3+)
- [ ] Make new preset available but not default
- [ ] Collect user feedback on JSON reliability
- [ ] Consider making schema-embedded the default for Perplexity if success rate significantly better

## Risk Assessment & Mitigation

### Risk 1: Schema Changes Break Prompt
**Impact**: Medium
**Probability**: Low
**Mitigation**:
- Schema is loaded dynamically from `deepsearch_results_schema.json`
- Changes to schema automatically reflected in generated prompts
- Version tracking in schema helps detect breaking changes

### Risk 2: Token Limit Issues
**Impact**: Medium
**Probability**: Medium
**Mitigation**:
- Full schema adds ~2000 tokens to prompt
- Monitor token usage in provider_params
- Consider schema compression for very long gene lists
- Document recommended gene list size limits (max 100-150 genes)

### Risk 3: Other Providers Affected
**Impact**: Low
**Probability**: Low
**Mitigation**:
- New template marked as `optimized_for: ["perplexity"]`
- Other presets (OpenAI, Consensus) continue using legacy templates
- Template selection is provider-aware via preset system

### Risk 4: Users Confused by Multiple Options
**Impact**: Low
**Probability**: Medium
**Mitigation**:
- Clear documentation distinguishing use cases
- `--list-presets` and `--list-templates` show descriptions
- Examples in documentation show recommended approaches
- Consider deprecation warning on legacy approach if new one proves superior

## Success Metrics

Track these metrics to evaluate success:

1. **JSON Success Rate**: % of API calls that return valid, parseable JSON
   - Target: >95% (vs current baseline)
2. **Schema Validation Rate**: % of JSON responses that pass schema validation
   - Target: >90%
3. **User Adoption**: % of Perplexity calls using new preset after 1 month
4. **Token Usage**: Average tokens per request (monitor for increase)
5. **API Errors**: Rate of 400/500 errors from Perplexity

## References

- Working example: `deep-research-client/tmp/prompt_0_Gliosis.md`
- Generation script: `deep-research-client/tmp/generate_table_s10_prompts.py`
- Current schema: `langpa/src/langpa/schemas/deepsearch_results_schema.json`
- Current templates: `langpa/src/langpa/services/deepsearch_prompts.py`
- Current presets: `langpa/src/langpa/services/deepsearch_configs.py`

## Open Questions

1. Should we eventually deprecate the system-prompt schema approach if schema-embedded proves more reliable?
2. Should we add token usage monitoring/warnings for very long gene lists?
3. Should schema-embedded become the default for all Perplexity presets?
4. Do we need separate templates for different schema versions, or is dynamic loading sufficient?

## Appendix A: File Change Summary

| File | Lines Changed | Type of Change |
|------|--------------|----------------|
| `deepsearch_prompts.py` | +60 | Add template, update function signature |
| `deepsearch_service.py` | +25 | Update prompt construction, system prompt logic |
| `deepsearch_configs.py` | +20 | Add new preset |
| **Total** | **~105 lines** | **3 files** |

## Appendix B: Template Comparison

### Legacy Template (gene_analysis_academic)
- Schema: In system prompt
- Genes: Comma-separated string
- Output instruction: Generic "respond with JSON"
- Optimized for: Perplexity, Consensus

### New Template (gene_analysis_schema_embedded)
- Schema: In user prompt (embedded)
- Genes: JSON array format
- Output instruction: Explicit with fallback guidance
- Optimized for: Perplexity (proven working)

Both templates share the same analysis strategy and guidelines, differing only in technical formatting and schema delivery mechanism.
